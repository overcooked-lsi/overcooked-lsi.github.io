<!doctype html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>On the Importance of Environments in Human-Robot Coordination</title><meta name="description" content="We demonstrate the effect of the environment on human-robot coordination and introduce a framework for finding environments that elicit different coordination behaviors."><link rel="canonical" href="https://overcooked-lsi.github.io"><meta property="og:title" content="On the Importance of Environments in Human-Robot Coordination"><meta property="og:site_name" content="On the Importance of Environments in Human-Robot Coordination"><meta property="og:description" content="We demonstrate the effect of the environment on human-robot coordination and introduce a framework for finding environments that elicit different coordination behaviors."><meta property="og:type" content="article"><meta property="og:url" content="https://overcooked-lsi.github.io"><meta property="og:image" content="https://overcooked-lsi.github.io/assets/img/open-graph-preview.png"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="On the Importance of Environments in Human-Robot Coordination"><meta name="twitter:description" content="We demonstrate the effect of the environment on human-robot coordination and introduce a framework for finding environments that elicit different coordination behaviors."><meta name="twitter:image" content="https://overcooked-lsi.github.io/assets/img/twitter-preview.png"><link rel="stylesheet" href="assets/css/katex.min.css"><link rel="stylesheet" href="assets/css/highlight.min.css"><style>:root{--primary:rgba(0, 0, 0, 0.8);--secondary:rgba(0, 0, 0, 0.6)}@media print{*,:after,:before{background:0 0!important;color:#000!important;box-shadow:none!important;text-shadow:none!important}html{font-size:12px!important}body{padding:20px!important}article,main{margin:0!important;padding:0!important;width:100%!important;max-width:100%!important}a[href]:after{content:" (" attr(href) ")"}abbr[title]:after{content:" (" attr(title) ")"}a.permalink{display:none}a[href^="#"]:after,a[href^="javascript:"]:after{content:""}blockquote,pre{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}figure,img,tr,video{page-break-inside:avoid}h2,h3,p{orphans:3;widows:3}h1,h2,h3{page-break-after:avoid}}html{color:var(--primary);font-family:Georgia,serif;font-weight:300;font-size:20px;line-height:1.45}body{margin:0;padding:0}main{margin:0;width:calc(100% - 80px);padding:40px}@media only screen and (max-width:500px){main{width:calc(100% - 30px);padding:15px}}article{max-width:48rem;width:100%;padding:0;margin:60px auto 0 auto}footer{background-color:#f2f2f2;text-align:center;margin:0;padding:30px 20px;width:calc(100% - 40px);font-size:.8rem}a,a:visited{color:var(--theme);text-decoration:none}a:active,a:focus,a:hover{border-bottom:2px solid var(--theme)}a.permalink{color:rgba(0,0,0,.2);text-decoration:none;border:0;font-size:80%}a.permalink:hover{color:rgba(0,0,0,.8);text-decoration:none;border:0}p{font-size:1rem;margin-bottom:1.3rem}p.description{font-style:italic;margin-top:0;margin-bottom:.5rem}code,pre{font-family:Menlo,Monaco,Consolas,Andale Mono,lucida console,"Courier New",monospace;background-color:#fafafa}pre{padding:.5rem;line-height:1.25;overflow-x:auto;font-size:.9rem}h1,h2,h3,h4,h5,h6{font-weight:400;line-height:1.42;margin:1.4rem 0 1.15rem 0;padding-bottom:.5rem;text-align:left}h1{margin-top:0;padding-bottom:0;margin-bottom:.5rem;font-size:2.4rem}h2{font-size:1.9rem}h3{font-size:1.5rem}h4{font-size:1.2rem;font-weight:700}h5{font-size:1.1rem}h6{font-size:.9rem}figure{padding:0;margin:0}img,video{width:60%;margin:0 auto;display:block}figcaption{margin:1rem 40px;font-size:.8rem;color:var(--secondary);text-align:center}@media only screen and (max-width:700px){img,video{width:100%}figcaption{margin-left:5px;margin-right:5px}}table{display:block;margin:0 auto;font-size:.7rem;width:100%}th{padding:0 .25rem}blockquote{border-left:8px solid #fafafa;padding:1rem}small{font-size:.707em}canvas,iframe,select,svg,textarea{max-width:100%}:root{--theme:#6495ed}::-moz-selection{background:rgba(0,0,0,.2)}::selection{background:rgba(0,0,0,.2)}.links{padding:6px 0;overflow:hidden}address{padding:6px 0;overflow:hidden;font-style:normal}.author-cell,.link-cell{float:left;display:inline-block;padding:8px 36px 8px 0}.author-cell a,.link-cell a{text-decoration:none}.link-cell a,.link-cell a:hover{font-size:1rem;color:var(--theme);border-color:var(--theme)}.link-cell a{font-weight:700}.author-affiliation,.author-name{display:block;padding:0;margin:0}.author-affiliation a{color:var(--primary)}.date{float:right;padding-right:0;color:var(--secondary)}@media only screen and (max-width:700px){.author-cell,.date,.link-cell{float:none;display:block}.links{padding-top:0}.date{padding-top:16px}}.modal-container{margin:0;position:relative}.modal-container.footnote{margin-right:.25rem}.modal-container button{display:inline;margin:0;padding:0;font-family:inherit;font-size:inherit;color:var(--theme);background:#fff;border:0}.modal-container .modal-backdrop{display:none}.modal-container .modal-toggle{position:absolute;left:0;top:0;height:100%;width:100%;margin:0;opacity:0;cursor:pointer}.modal-container .modal-toggle:hover~button{text-decoration:underline}.modal-container .modal-toggle:checked{width:100vw;height:100vh;position:fixed;left:0;top:0;z-index:9;opacity:0}.modal-container .modal-toggle:checked~.modal-backdrop{display:block;background-color:rgba(0,0,0,.1);width:100vw;height:100vh;position:fixed;left:0;top:0;z-index:9;pointer-events:none;opacity:1}.modal-container .modal-toggle:checked~.modal-backdrop .modal-content{display:block;background-color:rgba(255,255,255,.9);max-width:360px;width:100%;padding:15px;position:absolute;left:calc(50% - 200px);top:12%;z-index:999;pointer-events:auto;cursor:auto;box-shadow:0 3px 7px rgba(0,0,0,.3);font-style:normal;font-size:.7rem}@media only screen and (max-width:400px){.modal-container .modal-toggle:checked~.modal-backdrop .modal-content{left:0}}.modal-container .modal-toggle:checked~.modal-backdrop .modal-content .modal-close{display:block;padding-top:1rem;width:100%;text-align:right;color:var(--theme);font-size:14px;font-weight:400;cursor:pointer}.modal-container .modal-toggle:checked~.modal-backdrop .modal-content .modal-close:hover{text-decoration:underline}sup.footnote-number{font-weight:700}ol.end-list{padding-left:30px;font-size:.8rem}.end-item{margin-bottom:.7rem}.ref-link,.ref-link:visited{color:var(--theme);border-bottom:0}.ref-link:hover{color:var(--theme);border-bottom:0;text-decoration:underline}</style></head><body><main><article><h1>On the Importance of Environments in Human-Robot Coordination</h1><p class="description">We demonstrate the effect of the environment on human-robot coordination and introduce a framework for finding environments that elicit different coordination behaviors.</p><address><div class="author-cell"><span class="author-name"><a title="Matthew C. Fontaine*" href="https://www.linkedin.com/in/matthew-fontaine-53510b122/">Matthew C. Fontaine*</a> </span><span class="author-affiliation"><a title="USC" href="https://usc.edu">USC</a></span></div><div class="author-cell"><span class="author-name"><a title="Ya-Chuan Hsu*" href="https://www.linkedin.com/in/sophie-ya-chuan-hsu/">Ya-Chuan Hsu*</a> </span><span class="author-affiliation"><a title="USC" href="https://usc.edu">USC</a></span></div><div class="author-cell"><span class="author-name"><a title="Yulun Zhang*" href="https://yulunzhang.net/">Yulun Zhang*</a> </span><span class="author-affiliation"><a title="USC" href="https://usc.edu">USC</a></span></div><div class="author-cell"><span class="author-name"><a title="Bryon Tjanaka" href="https://btjanaka.net">Bryon Tjanaka</a> </span><span class="author-affiliation"><a title="USC" href="https://usc.edu">USC</a></span></div><div class="author-cell"><span class="author-name"><a title="Stefanos Nikolaidis" href="https://stefanosnikolaidis.net/">Stefanos Nikolaidis</a> </span><span class="author-affiliation"><a title="USC" href="https://usc.edu">USC</a></span></div></address><div class="links"><div class="link-cell"><a title="arXiv (TODO - also update link at bottom)" href="/">arXiv (TODO - also update link at bottom)</a></div><div class="link-cell"><a title="Discussion" href="https://github.com/overcooked-lsi/overcooked-lsi.github.io/discussions">Discussion</a></div></div><p><i>Estimated reading time: 10 min</i></p><html><head></head><body><p>In human-robot interaction (HRI), there has been much work on modeling human states and actions and then integrating such models into robot decision making. Researchers assess these models based on how well the human-robot team coordinates. However, few prior works have explored the effect of the environment on this coordination. Our thesis is that <em>changing the environment can result in significant differences between coordination behaviors.</em></p><p>Consider the following example of a human and a robot coordinating in the video game Overcooked, where a human-robot team prepares soups by gathering onions, cooking the onions into a soup on a stove, collecting the soup in a bowl, and finally delivering the soup to a counter (grey tile). In the video below, a robot (in green) with a QMDP <cite class="modal-container"><input id="ref-toggle-0" class="modal-toggle" type="checkbox"> <button>[1]</button> <span class="modal-backdrop"><span class="modal-content"><span class="end-item"><span><b>Shared autonomy via hindsight optimization</b> <a title="Link" href="https://www.ri.cmu.edu/pub_files/2015/7/Javdani15Hindsight.pdf" class="ref-link">[link]</a><br>Javdani, S., Srinivasa, S., Bagnell, J., 2015. <i>Robotics science and systems: online proceedings</i>.</span></span> <label class="modal-close" for="ref-toggle-0">[Close]</label></span></span></cite> policy collaborates with a rule-based simulated human (in blue) to prepare two soups as quickly as possible.</p><figure><video src="assets/img/fig6_4.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop="" preload="auto"></video><figcaption>A human (in blue) and robot with a QMDP policy (in green) collaborate to serve two soups. In this environment, the human and robot divide the work equally.</figcaption></figure><p>Observe the above environment layout during the coordination. The broadly open workspace allows both the human and robot to easily move around and, as such, the team can distribute workload equally between the human and robot. Next, contrast the layout above with the layout below, where the human does all the work. Keep in mind that the human and the robot follow identical policies in both scenarios — only the environment changes.</p><figure><video src="assets/img/fig6_1.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop="" preload="auto"></video><figcaption>A human and robot collaborate to serve two soups in a different environment, but this time, the human does all of the work.</figcaption></figure><p>Manually designing environments that induce specific coordination behaviors requires substantial human effort. Furthermore, as robotic systems increase in complexity, it becomes challenging to predict how these systems will act in different situations and even more challenging to design environments that elicit a diverse range of behaviors that are possible in the real world <em>before</em> we deploy the robot in the real world. <em>Therefore, we propose a framework for automatic environment generation, drawing upon insights from the field of procedural content generation in games.</em></p><h2 id="generating-solvable-environments-with-gan%2Bmip">Generating Solvable Environments with GAN+MIP <a class="permalink" href="#generating-solvable-environments-with-gan%2Bmip">¶</a></h2><style>div.envs {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    width: 100%;
    margin: 0px auto;
  }

  div.envs img {
    width: 32%;
    margin: 0.5%;
  }

  div.envs figcaption {
    margin: 1rem 0;
  }</style><p>Starting with this section, we will construct our proposed framework piece by piece, beginning with generating environments via a generative adversarial network (GAN). In Overcooked, we define an environment as a grid of tiles, where we represent each chosen tile as a one-hot vector indicating one of eight tile types. Examples of tile types include the starting agent locations, floor tiles, countertops, and stoves. To train our GAN, we manually authored a set of environments based on the environments from the Overcooked video game <cite class="modal-container"><input id="ref-toggle-1" class="modal-toggle" type="checkbox"> <button>[2]</button> <span class="modal-backdrop"><span class="modal-content"><span class="end-item"><span><b>Overcooked</b> <a title="Link" href="https://store.steampowered.com/app/448510/Overcooked/" class="ref-link">[link]</a><br>Ghost Town Games, 2016.</span></span> <label class="modal-close" for="ref-toggle-1">[Close]</label></span></span></cite>. Example environments from our human-authored training data are shown below.</p><figure><div class="envs"><img src="assets/img/human_generated/0.png" loading="lazy"> <img src="assets/img/human_generated/1.png" loading="lazy"> <img src="assets/img/human_generated/2.png" loading="lazy"></div><figcaption>Human-authored Overcooked environments.</figcaption></figure><p>Because we seek to create environments that are realistic and match the human authors’ style, we train a GAN to mimic the human-authored environments. Below we visualize our GAN architecture. The generator on the left-hand side takes as input a latent vector and outputs a one-hot encoded environment. Meanwhile, the discriminator on the right-hand side takes in the environment and outputs a score measuring the realism of a proposed environment.</p><figure><img style="width: 90%" src="assets/img/gan_architecture.png" loading="lazy" alt="GAN architecture for generating Overcooked
  environments."><figcaption>GAN architecture for generating Overcooked environments.</figcaption></figure><p>To generate original environments, we feed latent vectors into our fully-trained GAN. The figure below contains sample environments generated by our GAN model.</p><figure><div class="envs"><img src="assets/img/gan_generated/0.png" loading="lazy"> <img src="assets/img/gan_generated/1.png" loading="lazy"> <img src="assets/img/gan_generated/2.png" loading="lazy"></div><figcaption>Overcooked environments generated directly by a GAN. Each environment contains flaws that prevent any human-robot team from solving the environment. For instance, none of them have a robot (which would be a character with a green hat) and several of the environments contain key items not reachable by either player.</figcaption></figure><p>However, as shown above, many environments generated by the GAN are unsolvable. In particular:</p><ol><li>There may be too many of a given tile type (e.g. more than one human or robot).</li><li>Key objects like the stove may be unreachable.</li><li>The agents may be able to step outside of the environment (because there is no countertop at the edge).</li></ol><p>To make the environments solvable, we adapt ideas from <cite class="modal-container"><input id="ref-toggle-2" class="modal-toggle" type="checkbox"> <button>[3]</button> <span class="modal-backdrop"><span class="modal-content"><span class="end-item"><span><b>Video Game Level Repair via Mixed Integer Linear Programming</b> <a title="Link" href="https://ojs.aaai.org//index.php/AIIDE/article/view/7424" class="ref-link">[link]</a><br>Zhang*, H., Fontaine*, M., Hoover, A., Togelius, J., Dilkina, B., Nikolaidis, S., 2020. <i>Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</i>.</span></span> <label class="modal-close" for="ref-toggle-2">[Close]</label></span></span></cite> and repair each environment via a mixed-integer linear program (MIP). The MIP modifies each environment to satisfy solvability constraints while minimizing the edit distance between GAN-generated environments and repaired environments. In other words, the MIP finds a new environment similar to the GAN-authored one that also satisfies all solvability constraints for the human-robot team. The images below show the environments before (top row) and after (bottom row) the MIP repair.</p><figure><div class="envs"><img src="assets/img/gan_generated/0.png" loading="lazy"> <img src="assets/img/gan_generated/1.png" loading="lazy"> <img src="assets/img/gan_generated/2.png" loading="lazy"></div><hr><div class="envs"><img src="assets/img/gan_milp/0.png" loading="lazy"> <img src="assets/img/gan_milp/1.png" loading="lazy"> <img src="assets/img/gan_milp/2.png" loading="lazy"></div><figcaption><b>Top row:</b> Overcooked environments generated by a GAN.<br><b>Bottom row:</b> The same environments after the MIP repair. Each environment maintains similarity to its top row counterpart (i.e. the edit distance is small), but the repaired environment is now solvable by a human-robot team. For instance, each environment has both one robot and one human who can both reach the stove, onions, bowls, and counter.</figcaption></figure><h2 id="eliciting-diverse-behaviors-with-latent-space-illumination">Eliciting Diverse Behaviors with Latent Space Illumination <a class="permalink" href="#eliciting-diverse-behaviors-with-latent-space-illumination">¶</a></h2><p>The GAN+MIP pipeline creates a generative space of environments guaranteed to be solvable by our human-robot team. We can sample the generative space of environments by passing latent vectors to our GAN+MIP pipeline to generate each new environment. However, recall that our goal is to discover multiple environments that are diverse with respect to the emergent coordination behaviors. For this reason, we explore the latent space <cite class="modal-container"><input id="ref-toggle-3" class="modal-toggle" type="checkbox"> <button>[4]</button> <span class="modal-backdrop"><span class="modal-content"><span class="end-item"><span><b>Illuminating mario scenes in the latent space of a generative adversarial network</b> <a title="Link" href="https://arxiv.org/abs/2007.05674" class="ref-link">[link]</a><br>Fontaine, M., Liu, R., Togelius, J., Hoover, A., Nikolaidis, S., 2021. <i>In Proceedings of the AAAI Conference on Artificial Intelligence</i>.</span></span> <label class="modal-close" for="ref-toggle-3">[Close]</label></span></span></cite> of our GAN+MIP pipeline directly with CMA‑ME <cite class="modal-container"><input id="ref-toggle-4" class="modal-toggle" type="checkbox"> <button>[5]</button> <span class="modal-backdrop"><span class="modal-content"><span class="end-item"><span><b>Covariance matrix adaptation for the rapid illumination of behavior space</b> <a title="Link" href="https://arxiv.org/abs/1912.02400" class="ref-link">[link]</a><br>Fontaine, M., Togelius, J., Nikolaidis, S., Hoover, A., 2020. <i>Proceedings of the 2020 genetic and evolutionary computation conference</i>.</span></span> <label class="modal-close" for="ref-toggle-4">[Close]</label></span></span></cite>, a state-of-the-art quality diversity <cite class="modal-container"><input id="ref-toggle-5" class="modal-toggle" type="checkbox"> <button>[6]</button> <span class="modal-backdrop"><span class="modal-content"><span class="end-item"><span><b>Quality-Diversity Optimization: a novel branch of stochastic optimization</b> <a title="Link" href="https://arxiv.org/abs/2012.04322" class="ref-link">[link]</a><br>Chatzilygeroudis, K., Cully, A., Vassiliades, V., Mouret, J., 2020. <i>arXiv preprint arXiv:2012.04322</i>.</span></span> <label class="modal-close" for="ref-toggle-5">[Close]</label></span></span></cite> algorithm.</p><p>CMA‑ME fills an archive of solutions that are diverse with respect to a set of behavioral characteristics (BCs), which are specified coordination behaviors measurable during simulation. For example, in the workload distribution experiment in the next section, we select three BCs related to how the workload is distributed: the differences in the numbers of 1) bowls, 2) ingredients, and 3) orders handled by the robot and human. Running CMA‑ME results in an archive where each cell contains an environment that elicits specific behavior from the human-robot team. The following figure visualizes our complete framework:</p><figure><img src="assets/img/framework.png" alt="The framework for generating environments that elicit diverse behaviors from
human-robot teams." loading="lazy"><figcaption>The framework for generating environments that elicit diverse behaviors from human-robot teams.</figcaption></figure><p>In short, CMA‑ME samples latent vectors that we feed to our GAN to generate new candidate environments. Our MIP repairs each environment so it is solvable, and we simulate a human-robot team in each environment to determine the BCs (workload distribution of the three measured subtasks). In simulation, the robot executes a QMDP policy that reasons about the human’s next goal and we model the simulated human with a rule-based model. We assess the workload distribution of the team in each generated environment and update the archive. Finally, CMA‑ME observes how each new environment changes the archive of environments and updates its sampling distribution of latent vectors from this information. CMA‑ME adapts its sampling distribution of latent vectors to fill in the archive with diverse, high-quality environments.</p><h2 id="experimenting-with-workload-distribution">Experimenting with Workload Distribution <a class="permalink" href="#experimenting-with-workload-distribution">¶</a></h2><p>In our paper we run several experiments demonstrating different use cases for our proposed framework. In this section we explore the workload distribution experiment, where our framework discovers different subtask distributions defined in previous sections.<span class="modal-container footnote"> <input id="footnote-toggle-1" class="modal-toggle" type="checkbox"> <button><sup class="footnote-number">1</sup></button> <span class="modal-backdrop"><span class="modal-content">See the “Minimizing Performance” experiment for “Workload Distributions with Human-Aware Planning” from our paper. <label class="modal-close" for="footnote-toggle-1">[Close]</label> </span></span></span>Previous work has shown that the perceived robot’s contribution to the team is a crucial metric of fluency <cite class="modal-container"><input id="ref-toggle-6" class="modal-toggle" type="checkbox"> <button>[7]</button> <span class="modal-backdrop"><span class="modal-content"><span class="end-item"><span><b>Evaluating fluency in human–robot collaboration</b> <a title="Link" href="https://hrc2.io/papers/evaluating-fluency" class="ref-link">[link]</a><br>Hoffman, G., 2019. <i>IEEE Transactions on Human-Machine Systems</i>.</span></span> <label class="modal-close" for="ref-toggle-6">[Close]</label></span></span></cite>, and human-robot teaming experiments found that the degree to which participants were occupied affected their subjective assessment of the robot as a teammate <cite class="modal-container"><input id="ref-toggle-7" class="modal-toggle" type="checkbox"> <button>[8]</button> <span class="modal-backdrop"><span class="modal-content"><span class="end-item"><span><b>Computational design of mixed-initiative human–robot teaming that considers human factors: situational awareness, workload, and workflow preferences</b> <a title="Link" href="https://journals.sagepub.com/doi/10.1177/0278364916688255" class="ref-link">[link]</a><br>Gombolay, M., Bair, A., Huang, C., Shah, J., 2017. <i>The International journal of robotics research</i>.</span></span> <label class="modal-close" for="ref-toggle-7">[Close]</label></span></span></cite>.</p><p>Below we visualize several environments which our framework discovered during the workload distribution experiment. CMA‑ME found environments 1-3, which have even workload distributions (the human and robot do the same amount of work). We note that the large open area in each environment provides the robot and human ample space to move between key object locations. Moreover, the robot and human each have their own “workspace” that facilitate working separately in parallel, making the task of delivering soups easier. Now contrast environments 1-3 with environments 4-6, which each have uneven workload distributions. These environments force the human to take all subtasks. Each one consists of a single “cramped” workspace of key items — bowl dispensers, onion dispensers, the stove, and serving station. The robot struggles to enter the workspace to help the human. We validate these results from a simulated human by running a user study (described in the next section), where the robot collaborates with a real human instead of our simulated human.</p><style>div.results {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    width: 100%;
    margin: 0px auto;
  }

  div.results figure {
    width: 32%;
    margin: 5px 0.5%;
  }

  div.results img, div.results video {
    width: 100%;
  }

  div.results figcaption {
    margin: 0.25rem 0;
  }</style><figure><div class="results"><figure><img src="assets/img/user_study/w1.png" loading="lazy" alt="(1)"><figcaption>(1)</figcaption></figure><figure><img src="assets/img/user_study/w2.png" loading="lazy" alt="(2)"><figcaption>(2)</figcaption></figure><figure><img src="assets/img/user_study/w3.png" loading="lazy" alt="(3)"><figcaption>(3)</figcaption></figure><figure><img src="assets/img/user_study/w4.png" loading="lazy" alt="(4)"><figcaption>(4)</figcaption></figure><figure><img src="assets/img/user_study/w5.png" loading="lazy" alt="(5)"><figcaption>(5)</figcaption></figure><figure><img src="assets/img/user_study/w6.png" loading="lazy" alt="(6)"><figcaption>(6)</figcaption></figure></div><figcaption>Environments found by our framework in the workload distribution experiments. Environments 1-3 have even workload distributions (i.e. human and robot do roughly the same amount of work) while environments 4-6 have uneven workload distributions (i.e. either the human or the robot does almost all of the work).</figcaption></figure><h2 id="evaluating-environments-with-real-users">Evaluating Environments with Real Users <a class="permalink" href="#evaluating-environments-with-real-users">¶</a></h2><p>While our proposed framework finds environments that cause coordination problems between our simulated human and our QMDP robot policy, we still need to verify that discovered environments cause issues for real humans. To evaluate our generated environments, we ran an online user study where we asked participants to solve Overcooked environments with a robot collaborator. On environments with even and uneven workloads for simulated humans, we evaluated real human participants on their ability to coordinate with their robot partner. The results of the study confirmed that generated environments elicit similar collaboration behavior between a real human and the robot that were observed when our simulated human collaborated with the same robot. Below we show a sample interaction between a robot (always running the same QMDP policy) and a human user on environments discovered by our proposed framework.</p><figure><div class="results"><figure><video src="assets/img/user_study/w1.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop="" preload="auto"></video><figcaption>(1)</figcaption></figure><figure><video src="assets/img/user_study/w2.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop="" preload="auto"></video><figcaption>(2)</figcaption></figure><figure><video src="assets/img/user_study/w3.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop="" preload="auto"></video><figcaption>(3)</figcaption></figure><figure><video src="assets/img/user_study/w4.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop="" preload="auto"></video><figcaption>(4)</figcaption></figure><figure><video src="assets/img/user_study/w5.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop="" preload="auto"></video><figcaption>(5)</figcaption></figure><figure><video src="assets/img/user_study/w6.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop="" preload="auto"></video><figcaption>(6)</figcaption></figure></div><figcaption>Videos of a real user interacting with the robot (always with the QMDP policy) in the environments discovered by our framework. The interactions show that the human and robot share the work evenly in environments 1-3, while the human does nearly all the work in environments 4-6.</figcaption></figure><h2 id="conclusion">Conclusion <a class="permalink" href="#conclusion">¶</a></h2><p>We presented a framework for generating environments that affect coordination behavior between a simulated human and a robot, even though the simulated human and robot do not change their interaction policies between environments. We then confirm, through an online user study, that real humans behave similarly to their simulated counterparts on each discovered environment. Our framework helps confirm our thesis that environments can have a large impact on coordination behavior in human-robot collaborative settings. We envision our framework as a method to help evaluate human-robot coordination in the future, as well as a reliable tool to help practitioners debug or tune their coordination algorithms. Finally, we hope our work will guide future human-robot coordination research to consider the environment as a significant factor in coordination problems.</p><p><em>This article is based on work that will be presented at RSS 2021. To learn more, read our paper on <a href="#">arXiv</a>. For questions and comments, please visit our <a href="https://github.com/overcooked-lsi/overcooked-lsi.github.io/discussions">Github Discussions page</a>.</em></p><h2 id="footnotes">Footnotes <a class="permalink" href="#footnotes">¶</a></h2><ol id="footnote-list" class="end-list"><li class="end-item">See the “Minimizing Performance” experiment for “Workload Distributions with Human-Aware Planning” from our paper.</li></ol><h2 id="references">References <a class="permalink" href="#references">¶</a></h2><ol id="reference-list" class="end-list"><li class="end-item"><span><b>Shared autonomy via hindsight optimization</b> <a title="Link" href="https://www.ri.cmu.edu/pub_files/2015/7/Javdani15Hindsight.pdf" class="ref-link">[link]</a><br>Javdani, S., Srinivasa, S., Bagnell, J., 2015. <i>Robotics science and systems: online proceedings</i>.</span></li><li class="end-item"><span><b>Overcooked</b> <a title="Link" href="https://store.steampowered.com/app/448510/Overcooked/" class="ref-link">[link]</a><br>Ghost Town Games, 2016.</span></li><li class="end-item"><span><b>Video Game Level Repair via Mixed Integer Linear Programming</b> <a title="Link" href="https://ojs.aaai.org//index.php/AIIDE/article/view/7424" class="ref-link">[link]</a><br>Zhang*, H., Fontaine*, M., Hoover, A., Togelius, J., Dilkina, B., Nikolaidis, S., 2020. <i>Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</i>.</span></li><li class="end-item"><span><b>Illuminating mario scenes in the latent space of a generative adversarial network</b> <a title="Link" href="https://arxiv.org/abs/2007.05674" class="ref-link">[link]</a><br>Fontaine, M., Liu, R., Togelius, J., Hoover, A., Nikolaidis, S., 2021. <i>In Proceedings of the AAAI Conference on Artificial Intelligence</i>.</span></li><li class="end-item"><span><b>Covariance matrix adaptation for the rapid illumination of behavior space</b> <a title="Link" href="https://arxiv.org/abs/1912.02400" class="ref-link">[link]</a><br>Fontaine, M., Togelius, J., Nikolaidis, S., Hoover, A., 2020. <i>Proceedings of the 2020 genetic and evolutionary computation conference</i>.</span></li><li class="end-item"><span><b>Quality-Diversity Optimization: a novel branch of stochastic optimization</b> <a title="Link" href="https://arxiv.org/abs/2012.04322" class="ref-link">[link]</a><br>Chatzilygeroudis, K., Cully, A., Vassiliades, V., Mouret, J., 2020. <i>arXiv preprint arXiv:2012.04322</i>.</span></li><li class="end-item"><span><b>Evaluating fluency in human–robot collaboration</b> <a title="Link" href="https://hrc2.io/papers/evaluating-fluency" class="ref-link">[link]</a><br>Hoffman, G., 2019. <i>IEEE Transactions on Human-Machine Systems</i>.</span></li><li class="end-item"><span><b>Computational design of mixed-initiative human–robot teaming that considers human factors: situational awareness, workload, and workflow preferences</b> <a title="Link" href="https://journals.sagepub.com/doi/10.1177/0278364916688255" class="ref-link">[link]</a><br>Gombolay, M., Bair, A., Huang, C., Shah, J., 2017. <i>The International journal of robotics research</i>.</span></li></ol></body></html></article></main><footer>This work is released under the <a title="CC BY 4.0" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> license. Powered by Bryon Tjanaka's <a title="Instill" href="https://github.com/btjanaka/instill">Instill</a> template.</footer></body></html>